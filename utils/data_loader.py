"""
æ•°æ®åŠ è½½ä¸é¢„å¤„ç†æ¨¡å—
ç”¨äºä¸‹è½½å’Œå¤„ç† AAPL è‚¡ç¥¨æ•°æ®ï¼Œå¹¶ç”Ÿæˆç”¨äºè®­ç»ƒçš„æ‰¹æ¬¡æ•°æ®
"""
import numpy as np
import pandas as pd
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler


def load_yahoo_stock(ticker='AAPL', start='2010-01-01', end='2023-12-31', use_mock_data=False):
    """
    ä½¿ç”¨ yfinance ä¸‹è½½è‚¡ç¥¨æ•°æ®å¹¶è¿›è¡Œå½’ä¸€åŒ–
    
    å‚æ•°:
        ticker: str, è‚¡ç¥¨ä»£ç ï¼Œé»˜è®¤ä¸º 'AAPL'
        start: str, å¼€å§‹æ—¥æœŸ
        end: str, ç»“æŸæ—¥æœŸ
        use_mock_data: bool, æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼ˆAPI é™æµæ—¶ä½¿ç”¨ï¼‰
    
    è¿”å›:
        data_scaled: numpy array, å½’ä¸€åŒ–åçš„æ”¶ç›˜ä»·æ•°æ® (n_samples,)
        scaler: MinMaxScaler å¯¹è±¡, ç”¨äºåå½’ä¸€åŒ–
        raw_data: pandas Series, åŸå§‹æ”¶ç›˜ä»·æ•°æ®
    """
    if use_mock_data:
        print(f"âš ï¸ ä½¿ç”¨æ¨¡æ‹Ÿ {ticker} è‚¡ç¥¨æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰...")
        return _generate_mock_stock_data()
    
    print(f"æ­£åœ¨ä¸‹è½½ {ticker} è‚¡ç¥¨æ•°æ® ({start} åˆ° {end})...")
    
    try:
        # ä½¿ç”¨ yfinance ä¸‹è½½æ•°æ®
        stock_data = yf.download(ticker, start=start, end=end, progress=False)
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        if stock_data.empty or len(stock_data) == 0:
            print("\nâŒ æ•°æ®ä¸‹è½½å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ API é™æµæˆ–ç½‘ç»œé—®é¢˜ï¼‰")
            print("ğŸ’¡ è‡ªåŠ¨åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼...\n")
            return _generate_mock_stock_data()
        
        # æå–æ”¶ç›˜ä»·åˆ—
        close_prices = stock_data['Close'].values.reshape(-1, 1)
        
        # ä½¿ç”¨ MinMaxScaler å°†æ•°æ®å½’ä¸€åŒ–åˆ° (0, 1) èŒƒå›´
        scaler = MinMaxScaler(feature_range=(0, 1))
        data_scaled = scaler.fit_transform(close_prices).flatten()
        
        print(f"âœ… æ•°æ®ä¸‹è½½å®Œæˆï¼å…± {len(data_scaled)} ä¸ªæ•°æ®ç‚¹")
        print(f"åŸå§‹æ•°æ®èŒƒå›´: [{close_prices.min():.2f}, {close_prices.max():.2f}]")
        
        return data_scaled, scaler, stock_data['Close']
        
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        print("ğŸ’¡ è‡ªåŠ¨åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼...\n")
        return _generate_mock_stock_data()


def _generate_mock_stock_data():
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„è‚¡ç¥¨æ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºæˆ– API é™æµæ—¶ï¼‰
    
    è¿”å›:
        data_scaled: numpy array, å½’ä¸€åŒ–åçš„æ•°æ®
        scaler: MinMaxScaler å¯¹è±¡
        raw_data: pandas Series, åŸå§‹æ•°æ®
    """
    np.random.seed(42)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼šåŸºç¡€è¶‹åŠ¿ + éšæœºæ³¢åŠ¨
    n_samples = 3000
    t = np.arange(n_samples)
    
    # é•¿æœŸä¸Šå‡è¶‹åŠ¿
    trend = 100 + 0.05 * t
    
    # å‘¨æœŸæ€§æ³¢åŠ¨
    seasonal = 10 * np.sin(2 * np.pi * t / 365)
    
    # éšæœºå™ªå£°
    noise = np.random.randn(n_samples) * 5
    
    # ç»„åˆæˆæ¨¡æ‹Ÿè‚¡ä»·
    mock_prices = trend + seasonal + noise
    mock_prices = mock_prices.reshape(-1, 1)
    
    # å½’ä¸€åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    data_scaled = scaler.fit_transform(mock_prices).flatten()
    
    # åˆ›å»º pandas Series
    dates = pd.date_range(start='2010-01-01', periods=n_samples, freq='D')
    raw_data = pd.Series(mock_prices.flatten(), index=dates)
    
    print(f"âœ… ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼å…± {len(data_scaled)} ä¸ªæ•°æ®ç‚¹")
    print(f"åŸå§‹æ•°æ®èŒƒå›´: [{mock_prices.min():.2f}, {mock_prices.max():.2f}]")
    
    return data_scaled, scaler, raw_data


def create_sequences(data, seq_len=60):
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£æ–¹æ³•åˆ›å»ºåºåˆ—æ•°æ®
    
    å°†æ—¶é—´åºåˆ—æ•°æ®è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼š
    ç»™å®šè¿‡å» seq_len ä¸ªæ—¶é—´æ­¥çš„æ•°æ®ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„å€¼
    
    å‚æ•°:
        data: numpy array, shape=(n_samples,), æ—¶é—´åºåˆ—æ•°æ®
        seq_len: int, è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆå›çœ‹çª—å£å¤§å°ï¼‰
    
    è¿”å›:
        X: numpy array, shape=(n_sequences, seq_len, 1), è¾“å…¥åºåˆ—
        y: numpy array, shape=(n_sequences, 1), ç›®æ ‡å€¼
    
    ç¤ºä¾‹:
        data = [1, 2, 3, 4, 5, 6, 7], seq_len = 3
        X = [[1,2,3], [2,3,4], [3,4,5], [4,5,6]]
        y = [4, 5, 6, 7]
    """
    X, y = [], []
    
    for i in range(len(data) - seq_len):
        # è¾“å…¥åºåˆ—: data[i : i+seq_len]
        X.append(data[i : i + seq_len])
        # ç›®æ ‡å€¼: data[i+seq_len]
        y.append(data[i + seq_len])
    
    X = np.array(X)  # shape: (n_sequences, seq_len)
    y = np.array(y)  # shape: (n_sequences,)
    
    # å¢åŠ ç‰¹å¾ç»´åº¦
    X = np.expand_dims(X, axis=-1)  # shape: (n_sequences, seq_len, 1)
    y = np.expand_dims(y, axis=-1)  # shape: (n_sequences, 1)
    
    return X, y


def split_data(X, y, train_ratio=0.7, val_ratio=0.15):
    """
    å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    
    å‚æ•°:
        X: numpy array, è¾“å…¥åºåˆ—
        y: numpy array, ç›®æ ‡å€¼
        train_ratio: float, è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: float, éªŒè¯é›†æ¯”ä¾‹
    
    è¿”å›:
        X_train, y_train: è®­ç»ƒé›†
        X_val, y_val: éªŒè¯é›†
        X_test, y_test: æµ‹è¯•é›†
    """
    n_samples = len(X)
    
    # è®¡ç®—åˆ’åˆ†ç‚¹
    train_end = int(n_samples * train_ratio)
    val_end = int(n_samples * (train_ratio + val_ratio))
    
    # åˆ’åˆ†æ•°æ®
    X_train, y_train = X[:train_end], y[:train_end]
    X_val, y_val = X[train_end:val_end], y[train_end:val_end]
    X_test, y_test = X[val_end:], y[val_end:]
    
    print(f"\næ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(X_val)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
    
    return X_train, y_train, X_val, y_val, X_test, y_test


def batch_generator(X, y, batch_size=32, shuffle=True):
    """
    æ‰¹æ¬¡æ•°æ®ç”Ÿæˆå™¨
    
    å‚æ•°:
        X: numpy array, shape=(n_samples, seq_len, input_size)
        y: numpy array, shape=(n_samples, output_size)
        batch_size: int, æ‰¹æ¬¡å¤§å°
        shuffle: bool, æ˜¯å¦éšæœºæ‰“ä¹±
    
    Yields:
        X_batch: shape=(seq_len, input_size, batch_size)
        y_batch: shape=(output_size, batch_size)
    """
    n_samples = len(X)
    indices = np.arange(n_samples)
    
    if shuffle:
        np.random.shuffle(indices)
    
    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]
        
        X_batch = X[batch_indices]  # (batch_size, seq_len, input_size)
        y_batch = y[batch_indices]  # (batch_size, output_size)
        
        # è½¬ç½®ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
        X_batch = np.transpose(X_batch, (1, 2, 0))  # (seq_len, input_size, batch_size)
        y_batch = y_batch.T  # (output_size, batch_size)
        
        yield X_batch, y_batch


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    print("æµ‹è¯•æ•°æ®åŠ è½½å™¨:\n")
    
    # åŠ è½½æ•°æ®
    data_scaled, scaler, raw_data = load_yahoo_stock(ticker='AAPL', use_mock_data=True)
    
    # åˆ›å»ºåºåˆ—
    seq_len = 60
    X, y = create_sequences(data_scaled, seq_len=seq_len)
    print(f"\nåºåˆ—å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # åˆ’åˆ†æ•°æ®
    X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, y)
    
    # æµ‹è¯•æ‰¹æ¬¡ç”Ÿæˆå™¨
    print(f"\næµ‹è¯•æ‰¹æ¬¡ç”Ÿæˆå™¨ (batch_size=32):")
    batch_count = 0
    for X_batch, y_batch in batch_generator(X_train, y_train, batch_size=32):
        batch_count += 1
        if batch_count == 1:
            print(f"  æ‰¹æ¬¡å½¢çŠ¶: X_batch={X_batch.shape}, y_batch={y_batch.shape}")
    print(f"  æ€»æ‰¹æ¬¡æ•°: {batch_count}")
    
    print("\næ•°æ®åŠ è½½å™¨æµ‹è¯•æˆåŠŸ!")
